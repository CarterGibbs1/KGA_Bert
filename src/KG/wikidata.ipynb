{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\carte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\carte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from wikidata.client import Client\n",
    "from wikidata.entity import Entity\n",
    "from collections import defaultdict, deque\n",
    "import requests, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "MAX_LEVEL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Home', 'Mark', 'Zuckerberg']\n"
     ]
    }
   ],
   "source": [
    "text = \"Home of Mark Zuckerberg is great!\"\n",
    "def get_nouns(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    retVal = []\n",
    "    i = 0\n",
    "    while i < len(tags):\n",
    "        key, tag = tags[i]\n",
    "        if 'NN' in tag:\n",
    "            #current_noun = key\n",
    "            #j = i + 1\n",
    "            #while j < len(tags):\n",
    "            #    key_j, tag_j = tags[j]\n",
    "            #    if 'NN' in tag_j:\n",
    "            #        current_noun = ' '.join([current_noun, key_j])\n",
    "            #        j += 1\n",
    "            #    else:\n",
    "            #        i = j - 1\n",
    "            #        break\n",
    "            #retVal.append(current_noun)\n",
    "            #if j == len(tags):\n",
    "            #    return retVal\n",
    "            retVal.append(key)\n",
    "        i += 1\n",
    "    return retVal\n",
    "    \n",
    "print(get_nouns(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_id(item):\n",
    "    try:\n",
    "        response = requests.get(f'https://en.wikipedia.org/w/api.php?action=query&prop=pageprops&titles={item}&format=json')\n",
    "        wikidata_id = list(json.loads(response.text)['query']['pages'].values())[0]['pageprops']['wikibase_item']\n",
    "        return wikidata_id\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_np = np.load(r\"D:\\KG\\wiki_trans_v1_vec.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_embeddings(labels_file):\n",
    "\n",
    "    def __clean_str__(l : str):\n",
    "        if '@en' in l:\n",
    "            l = l.replace('@en', '')\n",
    "        l = l.replace('\"', '')\n",
    "        return l.lower()\n",
    "    \n",
    "    retVal = {}\n",
    "    with open(labels_file) as labels:\n",
    "        for line in labels:\n",
    "            curr_label, line_num = line.split('\\t')\n",
    "            line_num = int(line_num)\n",
    "            curr_label = __clean_str__(curr_label)\n",
    "            retVal[curr_label] = embeddings_np[line_num]\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_embedding = get_training_embeddings(r'C:\\Users\\carte\\OneDrive\\Desktop\\Datascience\\SDS\\KGA_Bert\\data\\KG_data\\english_labels.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains(noun):\n",
    "    return noun.title() in label_to_embedding or noun.lower() in label_to_embedding or noun.upper() in label_to_embedding\n",
    "\n",
    "def get_val(ent):\n",
    "    val = label_to_embedding.get(str(ent.label))\n",
    "    if val is None:\n",
    "        val = label_to_embedding.get(str(ent.label).lower())\n",
    "    if val is None:\n",
    "        val = label_to_embedding.get(str(ent.label).upper())\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(noun):\n",
    "    client = Client()\n",
    "    id = get_wikidata_id(noun)\n",
    "    e = client.get(id)\n",
    "    seen = set()\n",
    "    q = deque([(e, 0)])\n",
    "    averages = {}\n",
    "    num_elems = [0] * 3\n",
    "    while q:\n",
    "        ent, level = q.popleft()\n",
    "        if ent in seen or level > MAX_LEVEL:\n",
    "            continue\n",
    "        seen.add(ent)\n",
    "        try:\n",
    "            if isinstance(ent, Entity) and contains(str(ent.label)):\n",
    "                if level not in averages:\n",
    "                    averages[level] = get_val(ent)\n",
    "                else:\n",
    "                    averages[level] += get_val(ent)\n",
    "                num_elems[level] += 1\n",
    "\n",
    "                e = list(client.get(ent.id).values())[:min(len(e), 20)]\n",
    "                for ent in e:\n",
    "                    q.append((ent, level + 1))\n",
    "        except:\n",
    "            continue\n",
    "    return {level : np.vectorize(lambda vals : round(vals, 4))(averages[level] / num_elems[level]) for level in range(3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bfs('Pfdsajkldsajklgj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(r'C:\\Users\\carte\\OneDrive\\Desktop\\Datascience\\SDS\\KGA_Bert\\data\\glue_data\\SST-2\\train.tsv', sep='\\t', header=0)\n",
    "nouns = set()\n",
    "for sentence in data_train['sentence']:\n",
    "    for noun in get_nouns(sentence):\n",
    "        nouns.add(noun)\n",
    "\n",
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_with_embeddings = {word for word in nouns if word in label_to_embedding}\n",
    "nouns_with_embeddings, len(nouns_with_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.DataFrame(columns=['1st', '2nd', '3rd'])\n",
    "for noun in list(nouns_with_embeddings)[:3]:\n",
    "    entities = bfs(noun.title())\n",
    "    df2 = {'1st': [entities[0]], '2nd': [entities[1]], '3rd': [entities[2]]}\n",
    "    embeddings = pd.concat([embeddings, pd.DataFrame(df2)], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.DataFrame(columns=['1st', '2nd', '3rd'])\n",
    "embeddings['hello'] = \n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0, 1, 1]) + np.array([0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
