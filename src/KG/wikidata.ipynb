{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r'c:\\users\\carte\\appdata\\local\\programs\\python\\python310\\lib\\site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\carte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\carte\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from wikidata.client import Client\n",
    "from wikidata.entity import Entity\n",
    "from collections import deque\n",
    "import requests, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "MAX_LEVEL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    retVal = []\n",
    "    i = 0\n",
    "    while i < len(tags):\n",
    "        key, tag = tags[i]\n",
    "        if 'NN' in tag:\n",
    "            #current_noun = key\n",
    "            #j = i + 1\n",
    "            #while j < len(tags):\n",
    "            #    key_j, tag_j = tags[j]\n",
    "            #    if 'NN' in tag_j:\n",
    "            #        current_noun = ' '.join([current_noun, key_j])\n",
    "            #        j += 1\n",
    "            #    else:\n",
    "            #        i = j - 1\n",
    "            #        break\n",
    "            #retVal.append(current_noun)\n",
    "            #if j == len(tags):\n",
    "            #    return retVal\n",
    "            retVal.append(key)\n",
    "        i += 1\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_id(item):\n",
    "    try:\n",
    "        response = requests.get(f'https://en.wikipedia.org/w/api.php?action=query&prop=pageprops&titles={item}&format=json')\n",
    "        wikidata_id = list(json.loads(response.text)['query']['pages'].values())[0]['pageprops']['wikibase_item']\n",
    "        return wikidata_id\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 58.4 GiB for an array with shape (15682637000,) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\SDS\\KGA_Bert\\src\\KG\\wikidata.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/KG/wikidata.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m embeddings_np \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mD:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mSDS\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mKGA_Bert\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mKG_data\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mwiki_trans_v1_vec.npy\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\users\\carte\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\numpy\\lib\\npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mopen_memmap(file, mode\u001b[39m=\u001b[39mmmap_mode,\n\u001b[0;32m    454\u001b[0m                                   max_header_size\u001b[39m=\u001b[39mmax_header_size)\n\u001b[0;32m    455\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 456\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(fid, allow_pickle\u001b[39m=\u001b[39;49mallow_pickle,\n\u001b[0;32m    457\u001b[0m                                  pickle_kwargs\u001b[39m=\u001b[39;49mpickle_kwargs,\n\u001b[0;32m    458\u001b[0m                                  max_header_size\u001b[39m=\u001b[39;49mmax_header_size)\n\u001b[0;32m    459\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[39m# Try a pickle\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[1;32mc:\\users\\carte\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\numpy\\lib\\format.py:809\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    807\u001b[0m     \u001b[39mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m    808\u001b[0m         \u001b[39m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[1;32m--> 809\u001b[0m         array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49mfromfile(fp, dtype\u001b[39m=\u001b[39;49mdtype, count\u001b[39m=\u001b[39;49mcount)\n\u001b[0;32m    810\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    811\u001b[0m         \u001b[39m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[0;32m    812\u001b[0m         \u001b[39m# memory-intensive way.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[39m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[39m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[0;32m    822\u001b[0m         array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mndarray(count, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 58.4 GiB for an array with shape (15682637000,) and data type float32"
     ]
    }
   ],
   "source": [
    "embeddings_np = np.load(r\"D:\\SDS\\KGA_Bert\\data\\KG_data\\wiki_trans_v1_vec.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_embeddings(labels_file):\n",
    "\n",
    "    def __clean_str__(l : str):\n",
    "        if '@en' in l:\n",
    "            l = l.replace('@en', '')\n",
    "        l = l.replace('\"', '')\n",
    "        return l.lower()\n",
    "    \n",
    "    retVal = {}\n",
    "    with open(labels_file) as labels:\n",
    "        for line in labels:\n",
    "            curr_label, line_num = line.split('\\t')\n",
    "            line_num = int(line_num)\n",
    "            curr_label = __clean_str__(curr_label)\n",
    "            retVal[curr_label] = embeddings_np[line_num]\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\carte\\\\OneDrive\\\\Desktop\\\\Datascience\\\\SDS\\\\KGA_Bert\\\\data\\\\KG_data\\\\english_labels.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\SDS\\KGA_Bert\\src\\KG\\wikidata.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/KG/wikidata.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m label_to_embedding \u001b[39m=\u001b[39m get_training_embeddings(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mcarte\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mOneDrive\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDesktop\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDatascience\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mSDS\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mKGA_Bert\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mKG_data\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39menglish_labels.tsv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32md:\\SDS\\KGA_Bert\\src\\KG\\wikidata.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/KG/wikidata.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m l\u001b[39m.\u001b[39mlower()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/KG/wikidata.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m retVal \u001b[39m=\u001b[39m {}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/KG/wikidata.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(labels_file) \u001b[39mas\u001b[39;00m labels:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/KG/wikidata.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m labels:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/KG/wikidata.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         curr_label, line_num \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\carte\\\\OneDrive\\\\Desktop\\\\Datascience\\\\SDS\\\\KGA_Bert\\\\data\\\\KG_data\\\\english_labels.tsv'"
     ]
    }
   ],
   "source": [
    "label_to_embedding = get_training_embeddings(r'C:\\Users\\carte\\OneDrive\\Desktop\\Datascience\\SDS\\KGA_Bert\\data\\KG_data\\english_labels.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains(noun):\n",
    "    return noun.title() in label_to_embedding or noun.lower() in label_to_embedding or noun.upper() in label_to_embedding\n",
    "\n",
    "def get_val(ent):\n",
    "    val = label_to_embedding.get(str(ent.label))\n",
    "    if val is None:\n",
    "        val = label_to_embedding.get(str(ent.label).lower())\n",
    "    if val is None:\n",
    "        val = label_to_embedding.get(str(ent.label).upper())\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(noun):\n",
    "    client = Client()\n",
    "    id = get_wikidata_id(noun)\n",
    "    e = client.get(id)\n",
    "    seen = set()\n",
    "    q = deque([(e, 0)])\n",
    "    averages = {}\n",
    "    num_elems = [0] * 3\n",
    "    while q:\n",
    "        ent, level = q.popleft()\n",
    "        if ent in seen or level > MAX_LEVEL:\n",
    "            continue\n",
    "        seen.add(ent)\n",
    "        try:\n",
    "            if isinstance(ent, Entity) and contains(str(ent.label)):\n",
    "                if level not in averages:\n",
    "                    averages[level] = get_val(ent)\n",
    "                else:\n",
    "                    averages[level] += get_val(ent)\n",
    "                num_elems[level] += 1\n",
    "\n",
    "                e = list(client.get(ent.id).values())[:min(len(e), 20)]\n",
    "                for ent in e:\n",
    "                    q.append((ent, level + 1))\n",
    "        except:\n",
    "            continue\n",
    "    return {level : np.vectorize(lambda vals : round(vals, 4))(averages[level] / num_elems[level]) for level in range(3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(r'D:\\SDS\\KGA_Bert\\data\\glue_data\\SST-2\\train.tsv', sep='\\t', header=0)\n",
    "data_test = pd.read_csv(r'D:\\SDS\\KGA_Bert\\data\\glue_data\\SST-2\\dev.tsv', sep='\\t', header=0)\n",
    "\n",
    "\n",
    "nouns = set()\n",
    "for sentence in data_train['sentence']:\n",
    "    for noun in get_nouns(sentence):\n",
    "        nouns.add(noun)\n",
    "\n",
    "for sentence in data_test['sentence']:\n",
    "    for noun in get_nouns(sentence):\n",
    "        nouns.add(noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_with_embeddings = {word for word in nouns if word in label_to_embedding}\n",
    "nouns_with_embeddings, len(nouns_with_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "for noun in list(nouns_with_embeddings)[:3]:\n",
    "    entities = bfs(noun)\n",
    "    embeddings_dict[noun.lower()] = [entities[0], entities[1], entities[2]]\n",
    "\n",
    "pd.DataFrame.from_dict(embeddings_dict, orient='index', columns=['1', '2', '3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 186/10226 [00:37<32:50,  5.10it/s] "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for noun in tqdm(nouns):\n",
    "    client = Client()\n",
    "    id = get_wikidata_id(noun)\n",
    "    e = client.get(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
