{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wikidata.client'; 'wikidata' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\SDS\\KGA_Bert\\src\\KG\\wikidata.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/KG/wikidata.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/KG/wikidata.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwikidata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m Client\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/KG/wikidata.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwikidata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mentity\u001b[39;00m \u001b[39mimport\u001b[39;00m Entity\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/KG/wikidata.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m defaultdict, deque\n",
      "File \u001b[1;32md:\\SDS\\KGA_Bert\\src\\KG\\wikidata.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwikidata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m Client\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwikidata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mentity\u001b[39;00m \u001b[39mimport\u001b[39;00m Entity\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m deque\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wikidata.client'; 'wikidata' is not a package"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from wikidata.client import Client\n",
    "from wikidata.entity import Entity\n",
    "from collections import defaultdict, deque\n",
    "import requests, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "MAX_LEVEL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Home of Mark Zuckerberg is great!\"\n",
    "def get_nouns(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    retVal = []\n",
    "    i = 0\n",
    "    while i < len(tags):\n",
    "        key, tag = tags[i]\n",
    "        if 'NN' in tag:\n",
    "            #current_noun = key\n",
    "            #j = i + 1\n",
    "            #while j < len(tags):\n",
    "            #    key_j, tag_j = tags[j]\n",
    "            #    if 'NN' in tag_j:\n",
    "            #        current_noun = ' '.join([current_noun, key_j])\n",
    "            #        j += 1\n",
    "            #    else:\n",
    "            #        i = j - 1\n",
    "            #        break\n",
    "            #retVal.append(current_noun)\n",
    "            #if j == len(tags):\n",
    "            #    return retVal\n",
    "            retVal.append(key)\n",
    "        i += 1\n",
    "    return retVal\n",
    "    \n",
    "print(get_nouns(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_id(item):\n",
    "    try:\n",
    "        response = requests.get(f'https://en.wikipedia.org/w/api.php?action=query&prop=pageprops&titles={item}&format=json')\n",
    "        wikidata_id = list(json.loads(response.text)['query']['pages'].values())[0]['pageprops']['wikibase_item']\n",
    "        return wikidata_id\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_np = np.load(r\"D:\\SDS\\KGA_Bert\\data\\KG_data\\wikidata_translation_v1_vectors.npy\\wiki_trans_v1_vec.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_embeddings(labels_file):\n",
    "\n",
    "    def __clean_str__(l : str):\n",
    "        if '@en' in l:\n",
    "            l = l.replace('@en', '')\n",
    "        l = l.replace('\"', '')\n",
    "        return l.lower()\n",
    "    \n",
    "    retVal = {}\n",
    "    with open(labels_file) as labels:\n",
    "        for line in labels:\n",
    "            curr_label, line_num = line.split('\\t')\n",
    "            line_num = int(line_num)\n",
    "            curr_label = __clean_str__(curr_label)\n",
    "            retVal[curr_label] = embeddings_np[line_num]\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_embedding = get_training_embeddings(r'C:\\Users\\carte\\OneDrive\\Desktop\\Datascience\\SDS\\KGA_Bert\\data\\KG_data\\english_labels.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains(noun):\n",
    "    return noun.title() in label_to_embedding or noun.lower() in label_to_embedding or noun.upper() in label_to_embedding\n",
    "\n",
    "def get_val(ent):\n",
    "    val = label_to_embedding.get(str(ent.label))\n",
    "    if val is None:\n",
    "        val = label_to_embedding.get(str(ent.label).lower())\n",
    "    if val is None:\n",
    "        val = label_to_embedding.get(str(ent.label).upper())\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(noun):\n",
    "    client = Client()\n",
    "    id = get_wikidata_id(noun)\n",
    "    e = client.get(id)\n",
    "    seen = set()\n",
    "    q = deque([(e, 0)])\n",
    "    averages = {}\n",
    "    num_elems = [0] * 3\n",
    "    while q:\n",
    "        ent, level = q.popleft()\n",
    "        if ent in seen or level > MAX_LEVEL:\n",
    "            continue\n",
    "        seen.add(ent)\n",
    "        try:\n",
    "            if isinstance(ent, Entity) and contains(str(ent.label)):\n",
    "                if level not in averages:\n",
    "                    averages[level] = get_val(ent)\n",
    "                else:\n",
    "                    averages[level] += get_val(ent)\n",
    "                num_elems[level] += 1\n",
    "\n",
    "                e = list(client.get(ent.id).values())[:min(len(e), 20)]\n",
    "                for ent in e:\n",
    "                    q.append((ent, level + 1))\n",
    "        except:\n",
    "            continue\n",
    "    return {level : np.vectorize(lambda vals : round(vals, 4))(averages[level] / num_elems[level]) for level in range(3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bfs('Pfdsajkldsajklgj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(r'C:\\Users\\carte\\OneDrive\\Desktop\\Datascience\\SDS\\KGA_Bert\\data\\glue_data\\SST-2\\train.tsv', sep='\\t', header=0)\n",
    "nouns = set()\n",
    "for sentence in data_train['sentence']:\n",
    "    for noun in get_nouns(sentence):\n",
    "        nouns.add(noun)\n",
    "\n",
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_with_embeddings = {word for word in nouns if word in label_to_embedding}\n",
    "nouns_with_embeddings, len(nouns_with_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "for noun in list(nouns_with_embeddings)[:3]:\n",
    "    entities = bfs(noun)\n",
    "    embeddings_dict[noun.lower()] = [entities[0], entities[1], entities[2]]\n",
    "\n",
    "pd.DataFrame.from_dict(embeddings_dict, orient='index', columns=['1', '2', '3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
