{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for fine-tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gc import collect\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import cuda\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "\n",
    "data_train = pd.read_csv(r'D:\\SDS\\KGA_Bert\\data\\glue_data\\SST-2\\train.tsv', sep='\\t', header=0)\n",
    "data_dev = pd.read_csv(r'D:\\SDS\\KGA_Bert\\data\\glue_data\\SST-2\\dev.tsv', sep='\\t', header=0)\n",
    "data_train = data_train.sample(frac = 1, ignore_index=True)\n",
    "data_dev = data_dev.sample(frac = 1, ignore_index=True)\n",
    "\n",
    "embedding_table = pd.read_csv(r\"D:\\SDS\\KGA_Bert\\data\\KG_data\\embedding_table.csv\", index_col=0, delimiter='|',\n",
    "                    converters={\n",
    "                        '1' : lambda x: np.array(x.removeprefix('[').removesuffix(']').split(','), dtype='float'),\n",
    "                        '2' : lambda x: np.array(x.removeprefix('[').removesuffix(']').split(','), dtype='float'),\n",
    "                        '3' : lambda x: np.array(x.removeprefix('[').removesuffix(']').split(','), dtype='float')\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = len(max(data_train.sentence, key=len))\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_OUT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "train_data = pd.DataFrame({\n",
    "    'sentence' : data_train['sentence'].apply(str.strip),\n",
    "    'label' : data_train['label'].apply(int)\n",
    "}).reset_index()\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'sentence' : data_dev['sentence'].apply(str.strip),\n",
    "    'label' : data_dev['label'].apply(int)\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    retVal = []\n",
    "    i = 0\n",
    "    while i < len(tags):\n",
    "        key, tag = tags[i]\n",
    "        if 'NN' in tag:\n",
    "            retVal.append(key)\n",
    "        i += 1\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_size):\n",
    "        self.sentences = dataset['sentence']\n",
    "        self.labels = dataset['label']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.sentences[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_size,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        return {'sentences' : torch.tensor(inputs['input_ids'], dtype=torch.long, device=device),\n",
    "                'mask' : torch.tensor(inputs['attention_mask'], dtype=torch.long, device=device),\n",
    "                'token_type_ids': torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long, device=device),\n",
    "                'labels' : torch.tensor(self.labels[idx], dtype=torch.float, device=device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultBERTClass(torch.nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(DefaultBERTClass, self).__init__()\n",
    "        \n",
    "        self.bert_layer = BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\"\n",
    "            )\n",
    "        #self.dropout = torch.nn.Dropout(p=0.3)\n",
    "        self.hidd = torch.nn.Linear(self.bert_layer.config.hidden_size + 200, NUM_OUT)\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, text, attention_mask, token_type_ids):\n",
    "        embeddings = self.bert_layer(text, attention_mask = attention_mask)\n",
    "        pooler = embeddings[0][:, 0]\n",
    "        #dropout = self.dropout(pooler)\n",
    "\n",
    "        # Concat Embeddings\n",
    "        noun_embeddings = np.zeros(200)\n",
    "        length = 0\n",
    "        for noun in get_nouns(text):\n",
    "            if noun in embedding_table.index:\n",
    "                noun_embeddings += ((1/2) * embedding_table.loc[noun][1] + (1/3) * embedding_table.loc[noun][2] + (1/6) * embedding_table.loc[noun][3])\n",
    "                length += 1\n",
    "        \n",
    "        if length != 0:\n",
    "            noun_embeddings /= length\n",
    "\n",
    "        concat_layer = np.concatenate([pooler, noun_embeddings])\n",
    "\n",
    "        hidden = self.hidd(concat_layer)\n",
    "        \n",
    "        output = self.sig(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCELoss()(outputs, targets)\n",
    "\n",
    "def train(model, optimizer, data_loader):\n",
    "    model.train()\n",
    "    for data in tqdm(data_loader):\n",
    "        inputs = data['sentences']\n",
    "        mask = data['mask']\n",
    "        token_type_ids = data['token_type_ids']\n",
    "        targets = data['labels'].unsqueeze(1)\n",
    "\n",
    "        #print(inputs, mask)\n",
    "\n",
    "        outputs = model(inputs, mask, token_type_ids)\n",
    "\n",
    "        #print(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        #optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Memory optimization\n",
    "        del inputs, mask, token_type_ids, targets\n",
    "        collect()\n",
    "        with torch.cuda.device(device):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def validation(model, data_loader):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader):\n",
    "            inputs = data['sentences']\n",
    "            mask = data['mask']\n",
    "            token_type_ids = data['token_type_ids']\n",
    "            targets = data['labels'].unsqueeze(1)\n",
    "\n",
    "            outputs = model(inputs, mask, token_type_ids)\n",
    "\n",
    "            del inputs, mask, token_type_ids\n",
    "            collect()\n",
    "            with torch.cuda.device(device):\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            fin_outputs.extend(outputs)\n",
    "            fin_targets.extend(targets)\n",
    "\n",
    "    return torch.stack(fin_outputs), torch.stack(fin_targets)\n",
    "\n",
    "def get_accuracy(guess, targs):\n",
    "    guesses = (guess >= 0.5).cpu().numpy()\n",
    "    targets = (targs >= 0.5).cpu().numpy()\n",
    "    return accuracy_score(guesses, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 3.64MB/s]\n",
      "c:\\Users\\carte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\carte\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "training_data = CustomDataset(train_data, tokenizer, MAX_LEN)\n",
    "training_loader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "testing_data = CustomDataset(test_data, tokenizer, MAX_LEN)\n",
    "testing_loader = DataLoader(testing_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def live_plot(accuracies):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure()\n",
    "    plt.xlim(0, EPOCHS)\n",
    "    plt.ylim(0, 1)\n",
    "    x= [float(i) for i in range(len(accuracies))]\n",
    "    y= [float(i) for i in accuracies]\n",
    "    \n",
    "    if len(x) > 1:\n",
    "        plt.plot(x,y)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\SDS\\KGA_Bert\\src\\training\\train_aug_bert.ipynb Cell 13\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/training/train_aug_bert.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m validation_accuracies \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/training/train_aug_bert.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/training/train_aug_bert.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     loss \u001b[39m=\u001b[39m train(model, optimizer, training_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/training/train_aug_bert.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Loss:  \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/training/train_aug_bert.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     guess, targs \u001b[39m=\u001b[39m validation(model, testing_loader)\n",
      "\u001b[1;32md:\\SDS\\KGA_Bert\\src\\training\\train_aug_bert.ipynb Cell 13\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/training/train_aug_bert.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(model, optimizer, data_loader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/training/train_aug_bert.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/training/train_aug_bert.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm(data_loader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/training/train_aug_bert.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         inputs \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39msentences\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SDS/KGA_Bert/src/training/train_aug_bert.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         mask \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\carte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\notebook.py:233\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m unit_scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[0;32m    232\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39m*\u001b[39m unit_scale \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal\n\u001b[1;32m--> 233\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstatus_printer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp, total, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdesc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mncols)\n\u001b[0;32m    234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mpbar \u001b[39m=\u001b[39m proxy(\u001b[39mself\u001b[39m)\n\u001b[0;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\carte\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39m# if not total:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[39m# Prepare IPython progress bar\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39mif\u001b[39;00m IProgress \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# #187 #451 #558 #872\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[0;32m    109\u001b[0m \u001b[39mif\u001b[39;00m total:\n\u001b[0;32m    110\u001b[0m     pbar \u001b[39m=\u001b[39m IProgress(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mtotal)\n",
      "\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "model = DefaultBERTClass()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "validation_accuracies = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train(model, optimizer, training_loader)\n",
    "    print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "    guess, targs = validation(model, testing_loader)\n",
    "\n",
    "    #print(guess, targs)\n",
    "    \n",
    "    accuracy = get_accuracy(guess, targs)\n",
    "    validation_accuracies.append(accuracy)\n",
    "\n",
    "    live_plot(validation_accuracies)\n",
    "\n",
    "    print('accuracy on test set {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
